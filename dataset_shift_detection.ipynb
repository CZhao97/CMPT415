{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from deep_ensemble_keras import create_model\n",
    "import os,sys\n",
    "import json\n",
    "import ember\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from tensorflow import keras\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.spatial import distance\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import OrderedDict\n",
    "from reliability_diagrams import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dim_input = 2381, dim_output = 2, lr = 0.01):\n",
    "#     print(\"Building neural network\")\n",
    "    model = Sequential()\n",
    "    model.add(Dense(70, input_shape=(dim_input,), activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(70, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(dim_output, activation='softmax'))\n",
    "\n",
    "#     print(\"Compiling neural network\")\n",
    "    # compile the model\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def extract_data():\n",
    "    data_dir_2017 = 'new_ember_2017_2'\n",
    "\n",
    "    data_dir_2018 = 'new_ember2018'\n",
    "\n",
    "    X_test_2017, y_test_2017 = ember.read_vectorized_features(data_dir_2017, subset = 'test', feature_version=2)\n",
    "\n",
    "    X_test_2018, y_test_2018 = ember.read_vectorized_features(data_dir_2018, subset = 'test', feature_version=2)\n",
    "\n",
    "    return X_test_2017, y_test_2017, X_test_2018, y_test_2018\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    pred = np.argmax(y_hat, 1)\n",
    "    return (pred == y).mean()\n",
    "\n",
    "def get_result(y_hat):\n",
    "    return np.argmax(y_hat, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_for_deep_ensemble(X_test_2017, y_test_2017, X_test_2018, y_test_2018):\n",
    "    \n",
    "    acc_2017, acc_2018 = [], []\n",
    "    \n",
    "    for i in range(5):\n",
    "        \n",
    "        print(i)\n",
    "        \n",
    "        model = create_model()\n",
    "\n",
    "        model.load_weights(\"models_keras/model_{}.h5\".format(i))\n",
    "        \n",
    "        acc_2017.append(accuracy(model.predict(X_test_2017), y_test_2017))\n",
    "        \n",
    "        acc_2018.append(accuracy(model.predict(X_test_2018), y_test_2018))\n",
    "        \n",
    "    return acc_2017, acc_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_2017, y_test_2017, X_test_2018, y_test_2018 = extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_2017 = preprocessing.normalize(X_test_2017, norm='l2', axis = 0)\n",
    "X_test_2018 = preprocessing.normalize(X_test_2018, norm='l2', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=500)\n",
    "X_test_2017_pca = pca.fit_transform(X_test_2017)\n",
    "X_test_2018_pca = pca.fit_transform(X_test_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "WARNING:tensorflow:From C:\\Users\\czhao\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "acc_2017, acc_2018 = get_accuracy_for_deep_ensemble(X_test_2017, y_test_2017, X_test_2018, y_test_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.875135, 0.88449, 0.9446, 0.880875, 0.961215]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.80637, 0.81192, 0.81482, 0.83674, 0.8376]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% n = 5: 0.56327\n",
    "\n",
    "t_val, p_val = ks_2samp(acc_2017, acc_2018, mode = 'asymp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.013475889875863678)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_val,p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_test(dataset_one, dataset_two, sig_value):\n",
    "    p_vals = []\n",
    "\n",
    "    # For each dimension we conduct a separate KS test\n",
    "    for i in range(dataset_one.shape[1]):\n",
    "        feature_tr = dataset_one[:, i]\n",
    "        feature_te = dataset_two[:, i]\n",
    "\n",
    "        t_val, p_val = ks_2samp(feature_tr, feature_te)\n",
    "\n",
    "        p_vals.append(p_val)\n",
    "\n",
    "    # Apply the Bonferroni correction to bound the family-wise error rate. This can be done by picking the minimum\n",
    "    # p-value from all individual tests.\n",
    "    p_vals = np.array(p_vals)\n",
    "    p_val = min(np.min(p_vals), 1.0)\n",
    "\n",
    "    return p_val, sig_value/dataset_one.shape[1], p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed_p, alpha, p_vals = ks_test(X_test_2017,X_test_2018, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed_p, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMDStatistic:\n",
    "    r\"\"\"The *unbiased* MMD test of :cite:`gretton2012kernel`.\n",
    "    The kernel used is equal to:\n",
    "    .. math ::\n",
    "        k(x, x') = \\sum_{j=1}^k e^{-\\alpha_j\\|x - x'\\|^2},\n",
    "    for the :math:`\\alpha_j` proved in :py:meth:`~.MMDStatistic.__call__`.\n",
    "    Arguments\n",
    "    ---------\n",
    "    n_1: int\n",
    "        The number of points in the first sample.\n",
    "    n_2: int\n",
    "        The number of points in the second sample.\"\"\"\n",
    "\n",
    "    def __init__(self, n_1, n_2):\n",
    "        \n",
    "        self.n_1 = n_1\n",
    "        self.n_2 = n_2\n",
    "        \n",
    "        # The three constants used in the test.\n",
    "        self.a00 = 1. / (n_1 * (n_1 - 1))\n",
    "        self.a11 = 1. / (n_2 * (n_2 - 1))\n",
    "        self.a01 = - 1. / (n_1 * n_2)\n",
    "\n",
    "    def __call__(self, sample_1, sample_2, alphas, ret_matrix=False):\n",
    "        r\"\"\"Evaluate the statistic.\n",
    "        The kernel used is\n",
    "        .. math::\n",
    "            k(x, x') = \\sum_{j=1}^k e^{-\\alpha_j \\|x - x'\\|^2},\n",
    "        for the provided ``alphas``.\n",
    "        Arguments\n",
    "        ---------\n",
    "        sample_1: :class:`torch:torch.autograd.Variable`\n",
    "            The first sample, of size ``(n_1, d)``.\n",
    "        sample_2: variable of shape (n_2, d)\n",
    "            The second sample, of size ``(n_2, d)``.\n",
    "        alphas : list of :class:`float`\n",
    "            The kernel parameters.\n",
    "        ret_matrix: bool\n",
    "            If set, the call with also return a second variable.\n",
    "            This variable can be then used to compute a p-value using\n",
    "            :py:meth:`~.MMDStatistic.pval`.\n",
    "        Returns\n",
    "        -------\n",
    "        :class:`float`\n",
    "            The test statistic.\n",
    "        :class:`torch:torch.autograd.Variable`\n",
    "            Returned only if ``ret_matrix`` was set to true.\"\"\"\n",
    "        sample_12 = torch.cat((sample_1, sample_2), 0)\n",
    "        distances = pdist(sample_12, sample_12, norm=2)\n",
    "\n",
    "        kernels = None\n",
    "        for alpha in alphas:\n",
    "            kernels_a = torch.exp(- alpha * distances ** 2)\n",
    "            if kernels is None:\n",
    "                kernels = kernels_a\n",
    "            else:\n",
    "                kernels = kernels + kernels_a\n",
    "\n",
    "        k_1 = kernels[:self.n_1, :self.n_1]\n",
    "        k_2 = kernels[self.n_1:, self.n_1:]\n",
    "        k_12 = kernels[:self.n_1, self.n_1:]\n",
    "\n",
    "        mmd = (2 * self.a01 * k_12.sum() +\n",
    "               self.a00 * (k_1.sum() - torch.trace(k_1)) +\n",
    "               self.a11 * (k_2.sum() - torch.trace(k_2)))\n",
    "        if ret_matrix:\n",
    "            return mmd, kernels\n",
    "        else:\n",
    "            return mmd\n",
    "\n",
    "    def pval(self, distances, n_permutations=1000):\n",
    "        r\"\"\"Compute a p-value using a permutation test.\n",
    "        Arguments\n",
    "        ---------\n",
    "        matrix: :class:`torch:torch.autograd.Variable`\n",
    "            The matrix computed using :py:meth:`~.MMDStatistic.__call__`.\n",
    "        n_permutations: int\n",
    "            The number of random draws from the permutation null.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The estimated p-value.\"\"\"\n",
    "        if isinstance(distances, Variable):\n",
    "            distances = distances.data\n",
    "        return permutation_test_mat(distances.cpu().numpy(),\n",
    "                                    self.n_1, self.n_2,\n",
    "                                    n_permutations,\n",
    "                                    a00=self.a00, a11=self.a11, a01=self.a01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMD(dataset_one, dataset_two):\n",
    "\n",
    "    mmd_test = MMDStatistic(len(dataset_one), len(dataset_two))\n",
    "\n",
    "\n",
    "    all_dist = distance.cdist(dataset_one, dataset_two, 'euclidean')\n",
    "    \n",
    "    \n",
    "    median_dist = np.median(all_dist)\n",
    "\n",
    "    # Calculate MMD.\n",
    "    t_val, matrix = mmd_test(torch.autograd.Variable(torch.tensor(dataset_one)),\n",
    "                             torch.autograd.Variable(torch.tensor(dataset_two)),\n",
    "                             alphas=[1/median_dist], ret_matrix=True)\n",
    "    p_val = mmd_test.pval(matrix)\n",
    "\n",
    "\n",
    "    return p_val, np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 298. GiB for an array with shape (200000, 200000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-876e6142a3fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mMMD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_2017_pca\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_2018_pca\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-b827e8f459c8>\u001b[0m in \u001b[0;36mMMD\u001b[1;34m(dataset_one, dataset_two)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mall_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_one\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_two\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'euclidean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36mcdist\u001b[1;34m(XA, XB, metric, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2727\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"out\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2728\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2729\u001b[1;33m         \u001b[0mdm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2730\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2731\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 298. GiB for an array with shape (200000, 200000) and data type float64"
     ]
    }
   ],
   "source": [
    "MMD(X_test_2017_pca,X_test_2018_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
